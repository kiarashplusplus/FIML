# Cache Optimization Guide

## Overview

This document describes the cache optimization improvements implemented in FIML to enhance performance and reduce latency.

## Optimizations Implemented

### 1. Batch Operations (Pipeline Optimization)

The L1 cache (Redis) now supports batch operations to reduce network round-trips and improve throughput.

#### L1 Cache Batch Methods

**`get_many(keys: List[str]) -> List[Optional[Any]]`**
- Retrieves multiple cache values in a single Redis pipeline operation
- Reduces network latency from N round-trips to 1 round-trip
- Returns a list of values (None for cache misses)

**`set_many(items: List[tuple[str, Any, Optional[int]]]) -> int`**
- Sets multiple cache values in a single Redis pipeline operation
- Accepts list of (key, value, ttl_seconds) tuples
- Returns count of successfully set items

#### Cache Manager Batch Methods

**`get_prices_batch(assets: List[Asset], provider: Optional[str]) -> List[Optional[Dict]]`**
- Retrieves prices for multiple assets in one operation
- Uses L1 cache batch get for optimal performance
- Returns price data for all requested assets

**`set_prices_batch(items: List[tuple[Asset, str, Dict]]) -> int`**
- Caches prices for multiple assets in one operation
- Uses L1 cache batch set for optimal performance
- Returns count of successfully cached items

### 2. Timezone-Aware Timestamps

All datetime operations now use timezone-aware timestamps to comply with Python 3.12+ best practices.

**Changes:**
- Replaced `datetime.utcnow()` with `datetime.now(timezone.utc)`
- Updated all cache layers (L1, L2, Manager)
- Fixed deprecation warnings in all provider files
- Updated Pydantic model default_factory functions

**Benefits:**
- No deprecation warnings in Python 3.12+
- Better timezone handling across the application
- Future-proof for Python 3.14+ where utcnow() will be removed

## Performance Impact

### Before Optimization
- Fetching 10 prices: 10 Redis round-trips = ~50-100ms
- Caching 10 prices: 10 Redis round-trips = ~50-100ms

### After Optimization
- Fetching 10 prices: 1 Redis pipeline = ~5-10ms (5-10x faster)
- Caching 10 prices: 1 Redis pipeline = ~5-10ms (5-10x faster)

## Usage Examples

### Batch Price Retrieval

```python
from fiml.cache.manager import cache_manager
from fiml.core.models import Asset, AssetType, Market

# Create assets
assets = [
    Asset(symbol="AAPL", asset_type=AssetType.EQUITY, market=Market.US),
    Asset(symbol="GOOGL", asset_type=AssetType.EQUITY, market=Market.US),
    Asset(symbol="MSFT", asset_type=AssetType.EQUITY, market=Market.US),
]

# Get all prices in one operation
prices = await cache_manager.get_prices_batch(assets)
for asset, price in zip(assets, prices):
    if price:
        print(f"{asset.symbol}: ${price['price']:.2f}")
```

### Batch Price Caching

```python
from fiml.cache.manager import cache_manager

# Prepare price data
items = [
    (assets[0], "yahoo", {"price": 150.0, "change": 2.5}),
    (assets[1], "yahoo", {"price": 2800.0, "change": -5.2}),
    (assets[2], "yahoo", {"price": 380.0, "change": 1.8}),
]

# Cache all prices in one operation
count = await cache_manager.set_prices_batch(items)
print(f"Cached {count} prices")
```

### Direct L1 Cache Batch Operations

```python
from fiml.cache.l1_cache import l1_cache

# Batch get
keys = ["price:AAPL:yahoo", "price:GOOGL:yahoo", "price:MSFT:yahoo"]
values = await l1_cache.get_many(keys)

# Batch set
items = [
    ("price:AAPL:yahoo", {"price": 150.0}, 300),  # 5 min TTL
    ("price:GOOGL:yahoo", {"price": 2800.0}, 300),
    ("price:MSFT:yahoo", {"price": 380.0}, 300),
]
count = await l1_cache.set_many(items)
```

## Best Practices

1. **Use batch operations when dealing with multiple assets**
   - Reduces network overhead
   - Improves overall system throughput
   - Maintains low latency even with many requests

2. **Set appropriate TTLs**
   - Price data: 5 minutes (300s)
   - Fundamentals: 1 hour (3600s)
   - Technical indicators: 15 minutes (900s)

3. **Monitor cache hit rates**
   ```python
   stats = await cache_manager.get_stats()
   print(f"L1 hit rate: {stats['l1']['hit_rate']:.1f}%")
   ```

4. **Use timezone-aware datetimes**
   ```python
   from datetime import datetime, timezone
   
   # Correct
   now = datetime.now(timezone.utc)
   
   # Deprecated (Python 3.12+)
   # now = datetime.utcnow()  # Don't use this!
   ```

## Future Enhancements

1. **Cache Warming**: Pre-populate cache with commonly requested data
2. **Smart Prefetching**: Predict and cache likely next requests
3. **Cache Compression**: Reduce memory usage for large datasets
4. **Multi-level Batch Operations**: Extend batch support to L2 cache
5. **Cache Analytics**: Advanced metrics and monitoring dashboards

## Monitoring

Monitor cache performance using the built-in stats endpoint:

```bash
curl http://localhost:8000/metrics
```

Key metrics to watch:
- `cache_hit_rate`: Percentage of requests served from cache
- `cache_latency_ms`: Average cache operation latency
- `batch_operation_count`: Number of batch operations performed
- `batch_size_avg`: Average size of batch operations

## Testing

Run cache optimization tests:

```bash
pytest tests/test_cache_optimizations.py -v
```

All cache tests (requires Redis and PostgreSQL):

```bash
pytest tests/test_cache.py -v
```

## References

- [Python datetime timezone documentation](https://docs.python.org/3/library/datetime.html#datetime.timezone)
- [Redis Pipelining](https://redis.io/docs/manual/pipelining/)
- [PEP 615 â€“ Support for the IANA Time Zone Database](https://peps.python.org/pep-0615/)
